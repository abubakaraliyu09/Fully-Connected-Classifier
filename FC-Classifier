import os
import warnings
warnings.filterwarnings('ignore')
warnings.filterwarnings('always')
import numpy as np
import pandas as pd
import cv2
import keras
from sklearn.model_selection import KFold
from keras.applications.densenet import DenseNet121
from keras.preprocessing import image
from keras.applications.densenet import preprocess_input, decode_predictions
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D, Dropout
from keras.utils.np_utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.svm import SVC
import tensorflow as tf
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, confusion_matrix
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
warnings.filterwarnings(action='ignore', category=DeprecationWarning, message='`np.bool` is a deprecated alias')

def denoise_img(image):
    return cv2.fastNlMeansDenoisingColored(image,10,10,7,15)

dataset_dir = 'Binary_data/small_data'


def loadImages(data_dir):
    data = {}
    for c in ['0', '1']:
        data[c] = []
        if c == '0':
            label = 0
        elif c == '1':
            label = 1

        files = os.listdir(data_dir + '/' + c)
        #files = files[0:20]  # Maximum number objects for each class

        print("A sample object from {} class:".format(c))
        # plt.imshow(image.load_img(data_dir + '/' + c + '/' + files[4], target_size=(224, 224)))
        # plt.show()

        for f_name in files:
            img_path = data_dir + '/' + c + '/' + f_name

            img = image.load_img(img_path, target_size=(224, 224))
            x = image.img_to_array(img)
            x = np.expand_dims(x, axis=0)
            x = preprocess_input(x)


            data[c].append([x, label])

    return data
data = loadImages(dataset_dir)

'''
Spliting data into Train and Test (Validation) set
'''

train_classObj1, test_classObj1 = train_test_split(data['0'], test_size = 0.2)
train_classObj2, test_classObj2 = train_test_split(data['1'], test_size = 0.2)


print("The number of TRAIN objects in each class: ", len(train_classObj1))
print("The number of TEST objects in each class: ", len(test_classObj1))

train_objs = train_classObj1 + train_classObj2
test_objs = test_classObj1 + test_classObj2

train_data = [x[0] for x in train_objs]
train_label = [y[1] for y in train_objs]

test_data = [x[0] for x in test_objs]
test_label = [y[1] for y in test_objs]
'''
Compatible input dimension to feed into DenseNet mode in single mode to convert images to features for SVM
'''
X_train = np.array(train_data)
X_test = np.array(test_data)
'''
Compatible input dimension for fed into DenseNet Architecture in batch mode
'''
train_data = np.squeeze(X_train)
train_label = np.array(train_label)

test_data = np.squeeze(X_test)
test_label = np.array(test_label)

print("Input dimension: ", train_data.shape)

# Merge inputs and targets
inputs = np.concatenate((train_data, test_data), axis=0)
targets = np.concatenate((train_label, test_label), axis=0)

# define 10-fold cross validation test harness
kfold = KFold(n_splits=2, shuffle=True, random_state=0)
acc_per_fold = []
loss_per_fold = []
cvscores = []

# K-fold Cross Validation model evaluation
fold_no = 10
for train, test in kfold.split(train_data, train_label):
    #Pre-trained DenseNet Model
    base_model = DenseNet121(weights='imagenet', include_top=False)
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    #x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    #x = Dense(32, activation='relu')(x)
    predictions = Dense(2, activation='softmax')(x)

    model = Model(inputs=base_model.input, outputs=predictions)

    # Freeze all convolutional DenseNet layers and train only top layers (FC)
    for layer in base_model.layers:
        layer.trainable = False

    #model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])
    base_learning_rate = 0.01
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
              loss=keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['acc'])

    #model.summary()

    # Generate a print
    print('------------------------------------------------------------------------')
    print(f'Training for fold {fold_no} ...')

    history = model.fit(inputs[train], to_categorical(targets[train]),
                        epochs=50,
                        batch_size=2,verbose=1)

    # Generate generalization metrics
    scores = model.evaluate(train_data[test], to_categorical(train_label[test]), verbose=0)
    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1] * 100}%')
    acc_per_fold.append(scores[1] * 100)
    loss_per_fold.append(scores[0])

    # Increase fold number
    fold_no = fold_no + 1
    result = {'acc_per_fold': acc_per_fold, 'loss_per_fold': loss_per_fold}
    result_df = pd.DataFrame.from_dict(result)
    result_df.to_csv('results2.csv',index=False, header=True)


# == Provide average scores ==
print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(acc_per_fold)):
    print('------------------------------------------------------------------------')
    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')


acc = history.history['acc']
#val_acc = history.history['val_acc']
loss = history.history['loss']
#val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, loss, '--', label='Training loss')
#plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
#Accuracy
plt.plot(epochs, acc, '--', label='Training acc')
#plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.show()
